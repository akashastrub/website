---
title: "Deep Learning Specialisation - Part 1"
date: "2024-02-12"
categories: [deep learning]
---

# Introduction

A few months ago, I embarked on a quest to consolidate and extend my knowledge of Deep Learning. I was presented with a plethora of options, ranging from Ian Goodfellow's Deep Learning [book](https://www.deeplearningbook.org/) to Jeremy Howard's [fastai course](https://course.fast.ai/), and Andrew Ng's [Deep Learning Specialization course](https://www.coursera.org/specializations/deep-learning#credits), among others. However, I found a lack of consolidated opinions on the best starting point in this mature and crucial field. In this two-part blog series, I aim to fill that gap by sharing my insights on the course I eventually chose: Andrew Ng's Deep Learning Specialization.

# Motivation

Rather than reiterating the significance of Deep Learning — a fact that is evident from its impact on society, and of course the recent increases in tech companies' market capitalizations — I will focus on my personal motivations for undertaking this course:

In my experience as a Data Scientist, the crux of the problems often revolves around data availability, value creation, and delivery. However, I have recently found it invaluable to align with stakeholders on the core objective of a data science problem: are we seeking inference or prediction? Upon careful consideration, most problems fall into one of these categories, though the lines can occasionally be blurred. When it comes to "pure" prediction problems, where understanding and intuition are secondary, neural networks are often an unbeatable choice if the data is plentiful. Why should one restrict oneself?

My background is primarily in statistics and traditional Machine Learning. Until recently, I had only dabbled in neural networks, viewing them as limited except in well-defined prediction problems. However, the emergent properties of today's LLMs have challenged this belief. With sufficient data and hyperparameters, we might even infer causality from data and conduct inference exercises using these tools.

Working with a Deep Learning framework offers a wide range of options. Compared to traditional statistical and ML techniques, neural networks offer infinite design possibilities and immense flexibility to fit data. This can make it challenging to work with, but it also gives data scientists a creative platform to experiment with various models.

# Course Content

The Deep Learning Specialization is structured into five courses:

1.  Neural Networks and Deep Learning
2.  Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization
3.  Structuring Machine Learning Projects
4.  Convolutional Neural Networks
5.  Sequence Models

The first three courses, which I will discuss in this post, lay the foundation of Deep Learning. The principles covered here apply to any neural network you might train, regardless of the architecture type, or data modality. The final two courses delve into two critical applications of deep learning today: Convolutional Neural Nets & Computer Vision, and Sequence Models (encompassing Language & Temporal data), which I will cover in Part 2. So, what do the first three courses entail?

Course 1
Course 1, Neural Networks and Deep Learning, begins by introducing neural networks based on simple machine learning algorithms like logistic regression. It then delves into the intuition behind these models, introduces gradient descent, and explains how it fits into one forward/backward propagation learning pass. The iterative nature of training efficient neural networks is emphasized from the outset.

Course 2
Course 2, Improving Deep Neural Networks: Hyperparameter Tuning, Regularization, and Optimization, commences with a comprehensive discussion on various regularization techniques such as L1, L2 (weight decay), dropout regularization, data augmentation, and early stopping. The pros and cons of minibatching are introduced, which segways nicely into optimisation techniques, discussing gradient descent, exponentially weighted moving averages, and tying these together to describe gradient descent with momentum. RMSProp, ADAM, and learning rate decay are also discussed. The course also touches upon the nuances associated with these techniques, such as exploding/vanishing gradients, and introduces gradcheck. The course stands out for its valuable qualitative discussions interspersed between technical topics. For example, Ng introduces the unconventional concept of  "orthogonalization," encouraging us to view different goals (such as minimising bias vs minimising variance) in building a neural network as orthogonal, and to treat them as such. Granted, in a Deep Learning model, the direct correlation between these two properties might be less obvious than in more traditional techniques, but at least to me, that seemed farfetched.

Course 3
Course 3, Structuring ML Projects, is where Ng shares his practical insights from decades of experience in applied Deep Learning projects. This course extends to any statistical/ML techniques one might implement, covering topics like evaluation metrics, error analysis, mislabeled data, and data mismatch. It also introduces transfer learning, fine-tuning, multi-task learning, and end-to-end deep learning. Of the three, this is the course I would most recommend.

# Reflections

Overall, my experience with the Deep Learning Specialization course has been highly rewarding. Initially, I was hesitant due to the course's choice of TensorFlow as its Deep Learning framework, a selection that seemed unconventional given the rising popularity of PyTorch.

![](pytorch_vs_tensorflow_hugging_face.png){fig-align="center"}

![](pytorch_vs_tensorflow_pwc.png){fig-align="center"}

TODO: discuss images above
TODO: URL for image: https://www.assemblyai.com/blog/pytorch-vs-tensorflow-in-2023/

However, my apprehensions were quickly allayed when I discovered that the majority of the work from the first three courses is entirely library-agnostic, carried out manually using vectors and matrices. Diving into the details and avoiding common abstractions when learning a subject can significantly enhance understanding, an approach that proved beneficial in this case. This hands-on, foundational approach is a significant advantage of this course, as it equips learners with a deep understanding before introducing them to powerful tools like TensorFlow or PyTorch.

Moreover, Course 3, Structuring Machine Learning Projects, offered insights that I will carry forward not only when implementing Deep Learning models but also when executing any Machine Learning model. After nearly a decade of regular involvement in such projects, Ng's recommendations on evaluating projects based on a blend of "optimizing" and "satisficing" metrics were particularly enlightening. Often, when working on a project for stakeholders, there are lengthy and complex discussions about which model is "best". Numerous evaluation metrics are considered, each with varying degrees of importance, leading to debates about which model emerges as the "winner". Ng's pragmatic approach suggests agreeing in advance on a single metric to optimize and a list of metrics to "satisfice". Examples of the latter could include prediction times, cost of retraining, or false negatives. The former could then be a single metric that, assuming all nuanced metrics from the "satisfice" list are met, you would ideally want to optimize. I have since experimented with this framework and have seen fantastic results.

Lastly, as companies like Meta continue to release massive open-source networks such as Llama7B, an essential skill in the Deep Learning space will be the ability to adapt such models to one's specific use case. Most companies lack the resources to train such a model and likely always will. The AI victors among the non-monopolistic businesses that constitute the bulk of the world's economy will be those that can leverage these models effectively. Techniques like transfer learning and more niche methods, such as RAG for LLMs, come to mind. While Ng briefly covers these, I believe their importance is on the rise. More on that in Part 2.

