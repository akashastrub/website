[
  {
    "objectID": "posts/dl_course_part1/index.html",
    "href": "posts/dl_course_part1/index.html",
    "title": "Deep Learning Specialisation - Part 1",
    "section": "",
    "text": "Introduction\nA few months ago, I embarked on a quest to consolidate and extend my knowledge of Deep Learning. I was presented with a plethora of options, ranging from Ian Goodfellow’s Deep Learning book to Jeremy Howard’s fastai course, and Andrew Ng’s Deep Learning Specialization course, among others. However, I found a lack of consolidated opinions on the best starting point in this mature and crucial field. In this two-part blog series, I aim to fill that gap by sharing my insights on the course I eventually chose: Andrew Ng’s Deep Learning Specialization.\n\n\nMotivation\nRather than reiterating the significance of Deep Learning — a fact that is evident from its impact on society, and of course the recent increases in tech companies’ market capitalizations — I will focus on my personal motivations for undertaking this course:\nIn my experience as a Data Scientist, the crux of the problems often revolves around data availability, value creation, and delivery. However, I have recently found it invaluable to align with stakeholders on the core objective of a data science problem: are we seeking inference or prediction? Upon careful consideration, most problems fall into one of these categories, though the lines can occasionally be blurred. When it comes to “pure” prediction problems, where understanding and intuition are secondary, neural networks are often an unbeatable choice if the data is plentiful. Why should one restrict oneself?\nMy background is primarily in statistics and traditional Machine Learning. Until recently, I had only dabbled in neural networks, viewing them as limited except in well-defined prediction problems. However, the emergent properties of today’s LLMs have challenged this belief. With sufficient data and hyperparameters, we might even infer causality from data and conduct inference exercises using these tools.\nWorking with a Deep Learning framework offers a wide range of options. Compared to traditional statistical and ML techniques, neural networks offer infinite design possibilities and immense flexibility to fit data. This can make it challenging to work with, but it also gives data scientists a creative platform to experiment with various models.\n\n\nCourse Content\nThe Deep Learning Specialization is structured into five courses:\n\nNeural Networks and Deep Learning\nImproving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization\nStructuring Machine Learning Projects\nConvolutional Neural Networks\nSequence Models\n\nThe first three courses, which I will discuss in this post, lay the foundation of Deep Learning. The principles covered here apply to any neural network you might train, regardless of the architecture type, or data modality. The final two courses delve into two critical applications of deep learning today: Convolutional Neural Nets & Computer Vision, and Sequence Models (encompassing Language & Temporal data), which I will cover in Part 2. So, what do the first three courses entail?\n\nCourse 1\nCourse 1, Neural Networks and Deep Learning, begins by introducing neural networks based on simple machine learning algorithms like logistic regression. It then delves into the intuition behind these models, introduces gradient descent, and explains how it fits into one forward/backward propagation learning pass. The iterative nature of training efficient neural networks is emphasized from the outset.\n\n\nCourse 2\nCourse 2, Improving Deep Neural Networks: Hyperparameter Tuning, Regularization, and Optimization, commences with a comprehensive discussion on various regularization techniques such as L1, L2 (weight decay), dropout regularization, data augmentation, and early stopping. The pros and cons of minibatching are introduced, which segways nicely into optimisation techniques, discussing gradient descent, exponentially weighted moving averages, and tying these together to describe gradient descent with momentum. RMSProp, ADAM, and learning rate decay are also discussed. The course also touches upon the nuances associated with these techniques, such as exploding/vanishing gradients, and introduces gradcheck. The course stands out for its valuable qualitative discussions interspersed between technical topics. For example, Ng introduces the unconventional concept of “orthogonalization,” encouraging us to view different goals (such as minimising bias vs minimising variance) in building a neural network as orthogonal, and to treat them as such. Granted, in a Deep Learning model, the direct correlation between these two properties might be less obvious than in more traditional techniques, but at least to me, that seemed farfetched.\n\n\nCourse 3\nCourse 3, Structuring ML Projects, is where Ng shares his practical insights from decades of experience in applied Deep Learning projects. This course extends to any statistical/ML techniques one might implement, covering topics like evaluation metrics, error analysis, mislabeled data, and data mismatch. It also introduces transfer learning, fine-tuning, multi-task learning, and end-to-end deep learning. Of the three, this is the course I would most recommend.\n\n\n\nReflections\nOverall, my experience with the Deep Learning Specialization course has been highly rewarding. Initially, I was hesitant due to the course’s choice of TensorFlow as its Deep Learning framework, a selection that seemed unconventional given the rising popularity of PyTorch (the figures below speak for themselves)…\n\n\n\nPercentage of repositories uploaded to paperswithcode.com, by Deep Learning framework\n\n\n\n\n\nNumber of models uploaded to HuggingFace, by Deep Learning framework\n\n\nHowever, my apprehensions were quickly allayed when I discovered that the majority of the work from the first three courses is entirely library-agnostic, carried out manually using vectors and matrices. Diving into the details and avoiding common abstractions when learning a subject can significantly enhance understanding, an approach that proved beneficial in this case. This hands-on, foundational approach is a significant advantage of this course, as it equips learners with a deep understanding before introducing them to powerful tools like TensorFlow or PyTorch.\nMoreover, Course 3, Structuring Machine Learning Projects, offered insights that I will carry forward not only when implementing Deep Learning models but also when executing any Machine Learning model. After nearly a decade of regular involvement in such projects, Ng’s recommendations on evaluating projects based on a blend of “optimizing” and “satisficing” metrics were particularly enlightening. Often, when working on a project for stakeholders, there are lengthy and complex discussions about which model is “best”. Numerous evaluation metrics are considered, each with varying degrees of importance, leading to debates about which model emerges as the “winner”. Ng’s pragmatic approach suggests agreeing in advance on a single metric to optimize and a list of metrics to “satisfice”. Examples of the latter could include prediction times, cost of retraining, or false negatives. The former could then be a single metric that, assuming all nuanced metrics from the “satisfice” list are met, you would ideally want to optimize. I have since experimented with this framework and have seen fantastic results.\nLastly, as companies like Meta continue to release massive open-source networks such as Llama7B, an essential skill in the Deep Learning space will be the ability to adapt such models to one’s specific use case. Most companies lack the resources to train such a model and likely always will. The AI victors among the non-monopolistic businesses that constitute the bulk of the world’s economy will be those that can leverage these models effectively. Techniques like transfer learning and more niche methods, such as RAG for LLMs, come to mind. While Ng briefly covers these, I believe their importance is on the rise. More on that in Part 2."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Deep Learning Specialisation - Part 1\n\n\n\n\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nFeb 18, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! I’m Akasha, a data scientist currently working at Novo Nordisk in Boston, USA. My current work focuses on the Digital Health space, but this personal website focuses on anything that inspires me to write in my free time."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Hello and welcome to my blog! The intention with this blog is both to provide an outlet for myself to consolidate my learning, and to share it with you in the hope that you can hopefully also learn something new. In terms of content, my current work focuses on the pharmaceutical space, but anything that inspires me to write and code in my free time will end up here. Three initial themes are currently in the pipeline:\n\nBayesian statistics: I often find the Bayesian framework more intuitive and useful than the more traditional frequentist framework. I can finally write about it here instead of bothering my colleagues about it.\nPersonal finance: Given I seem to only move to countries with different currencies to the one I was previously in, I have created a personal accounting tool that helps expats keep a tab on their income and expenses across different currencies. Friends & family already use this, but I will share it (and the development process) more broadly here.\nCausal inference: How do we use our knowledge, and statistics, to draw causal conclusions based on data? This is both the bread and butter of a lot of work in the pharmaceutical industry, as well as a very stimulating field in and of itself.\n\nOf course, as time goes by, new ideas will pop up, but it’s safe to say these will mainly focus on the pharmaceutical industry, data science, ML/MLOps, and the intersection of all of these. There will also be a few purely intellectual pursuits, such as book reviews. These will provide the content for my (un)ambitious goal of posting at least once per quarter, and I’m excited to put pen to paper (or rather, fingers to keyboard) and begin. Stay tuned."
  }
]